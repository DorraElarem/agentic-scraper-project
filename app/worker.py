#!/usr/bin/env python3
"""
Worker Celery pour l'agentic scraper
Point d'entr√©e principal pour le worker avec gestion d'erreurs et diagnostics
"""

import sys
import os
import logging
from datetime import datetime
from app.config.settings import settings

# Configuration des logs pour le worker
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Fonction principale du worker avec diagnostics int√©gr√©s"""
    logger.info(f"üöÄ WORKER STARTING AT {datetime.utcnow()}")
    
    try:
        # Test des imports critiques avant de d√©marrer Celery
        logger.info("üîß Testing critical imports...")
        
        # Test 1: Configuration
        try:
            from app.config.settings import settings
            logger.info(f"‚úÖ Settings imported - DEFAULT_DELAY: {settings.DEFAULT_DELAY}")
        except Exception as e:
            logger.error(f"‚ùå Settings import failed: {e}")
            return 1
        
        # Test 2: Scrapers avec gestion des attributs optionnels
        try:
            from app.scrapers.traditional import TunisianWebScraper
            from app.scrapers.intelligent import IntelligentScraper
            
            # Test de cr√©ation d'instances
            traditional = TunisianWebScraper()
            intelligent = IntelligentScraper()
            
            # üî• CORRECTION: Utiliser get_scraper_info() pour √©viter les AttributeError
            traditional_info = traditional.get_scraper_info()
            logger.info(f"‚úÖ Traditional scraper: delay={traditional_info.get('delay', 'unknown')}, max_length={traditional_info.get('max_content_length', 'unknown')}")
            
            # Pour intelligent scraper, utiliser getattr avec fallback
            intelligent_delay = getattr(intelligent, 'delay', 'unknown')
            logger.info(f"‚úÖ Intelligent scraper: delay={intelligent_delay}")
            
        except Exception as e:
            logger.error(f"‚ùå Scrapers creation failed: {e}")
            import traceback
            traceback.print_exc()
            return 1
        
        # Test 3: Agent scraper
        try:
            from app.agents.scraper_agent import ScraperAgent
            agent = ScraperAgent("test_worker_agent")
            logger.info("‚úÖ ScraperAgent created successfully")
        except Exception as e:
            logger.error(f"‚ùå ScraperAgent creation failed: {e}")
            import traceback
            traceback.print_exc()
            return 1
        
        # Test 4: T√¢ches de scraping avec nouvelle structure
        try:
            # Import du module sans importer la fonction directement
            import app.tasks.scraping_tasks
            
            # Utiliser l'enregistrement des t√¢ches
            from app.tasks.scraping_tasks import register_tasks
            tasks = register_tasks()
            
            logger.info("‚úÖ Scraping tasks imported successfully")
        except Exception as e:
            logger.error(f"‚ùå Scraping tasks import failed: {e}")
            import traceback
            traceback.print_exc()
            return 1
        
        # Test 5: Import de l'app Celery
        try:
            from app.celery_app import celery_app
            logger.info("‚úÖ Celery app imported successfully")
        except Exception as e:
            logger.error(f"‚ùå Celery app import failed: {e}")
            import traceback
            traceback.print_exc()
            return 1
        
        # V√©rification des t√¢ches enregistr√©es
        registered_tasks = list(celery_app.tasks.keys())
        logger.info(f"üìã Registered tasks: {len(registered_tasks)}")
        
        important_tasks = [
            'app.tasks.scraping_tasks.enqueue_scraping_task',
            'app.celery_app.test_task',
            'app.celery_app.debug_communication'
        ]
        
        missing_tasks = []
        for task in important_tasks:
            if task in registered_tasks:
                logger.info(f"‚úÖ Task available: {task}")
            else:
                logger.warning(f"‚ùå Task missing: {task}")
                missing_tasks.append(task)
        
        if missing_tasks:
            logger.error(f"‚ùå Critical tasks missing: {missing_tasks}")
            logger.info("üìã Available tasks:")
            for task in sorted(registered_tasks):
                logger.info(f"   - {task}")
            return 1
        
        # Test de connectivit√© Redis
        try:
            import redis
            redis_url = os.getenv('REDIS_URL', 'redis://redis:6379/0')
            r = redis.from_url(redis_url)
            r.ping()
            logger.info(f"‚úÖ Redis connection OK: {redis_url}")
        except Exception as e:
            logger.error(f"‚ùå Redis connection failed: {e}")
            return 1
        
        logger.info("‚úÖ ALL DIAGNOSTICS PASSED - Starting Celery worker...")
        
        # D√©marrer le worker Celery
        celery_app.start()
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("‚ÑπÔ∏è Worker stopped by user")
        return 0
    except Exception as e:
        logger.error(f"üí• Worker startup failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

def run_diagnostics_only():
    """Lance seulement les diagnostics sans d√©marrer le worker"""
    logger.info("üîç RUNNING DIAGNOSTICS ONLY...")
    
    try:
        # Import et test de tous les composants
        from app.config.settings import settings
        from app.scrapers.traditional import TunisianWebScraper
        from app.scrapers.intelligent import IntelligentScraper
        from app.agents.scraper_agent import ScraperAgent
        from app.models.schemas import ScrapeRequest, AnalysisType
        import app.tasks.scraping_tasks
        from app.celery_app import celery_app
        
        # Test de cr√©ation d'instances avec gestion d'erreurs
        traditional = TunisianWebScraper()
        intelligent = IntelligentScraper()
        agent = ScraperAgent("diagnostic_agent")
        
        # Test d'une requ√™te de scraping
        request = ScrapeRequest(
            urls=["https://httpbin.org/json"], 
            analysis_type=AnalysisType.STANDARD
        )
        
        logger.info("‚úÖ ALL COMPONENTS LOADED SUCCESSFULLY")
        
        # üî• CORRECTION: Utiliser les m√©thodes s√©curis√©es pour obtenir les infos
        try:
            traditional_info = traditional.get_scraper_info()
            logger.info(f"‚úÖ Traditional scraper: delay={traditional_info.get('delay', 'unknown')}, max_length={traditional_info.get('max_content_length', 'unknown')}")
        except Exception as e:
            # Fallback si get_scraper_info n'existe pas
            delay = getattr(traditional, 'delay', 'unknown')
            max_length = getattr(traditional, 'max_content_length', 'unknown')
            logger.info(f"‚úÖ Traditional scraper: delay={delay}, max_length={max_length}")
        
        intelligent_delay = getattr(intelligent, 'delay', 'unknown')
        logger.info(f"‚úÖ Intelligent scraper: delay={intelligent_delay}")
        
        agent_name = getattr(agent, 'name', 'unknown')
        logger.info(f"‚úÖ Agent: {agent_name}")
        
        logger.info(f"‚úÖ Test request: {len(request.urls)} URLs, type={request.analysis_type}")
        
        # V√©rifier les t√¢ches Celery
        registered_tasks = list(celery_app.tasks.keys())
        logger.info(f"‚úÖ Celery tasks: {len(registered_tasks)}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå DIAGNOSTIC FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_extended_diagnostics():
    """Diagnostics √©tendus avec tests approfondis"""
    logger.info("üî¨ RUNNING EXTENDED DIAGNOSTICS...")
    
    try:
        # Test des imports avec informations d√©taill√©es
        logger.info("üì¶ Testing module imports...")
        
        # Configuration
        from app.config.settings import settings
        logger.info(f"   ‚úÖ Settings: DEFAULT_DELAY={settings.DEFAULT_DELAY}, TIMEOUT={settings.REQUEST_TIMEOUT}")
        
        # Scrapers
        from app.scrapers.traditional import TunisianWebScraper
        from app.scrapers.intelligent import IntelligentScraper
        
        traditional = TunisianWebScraper()
        intelligent = IntelligentScraper()
        
        # Test des m√©thodes cl√©s
        traditional_methods = [method for method in dir(traditional) if not method.startswith('_')]
        intelligent_methods = [method for method in dir(intelligent) if not method.startswith('_')]
        
        logger.info(f"   ‚úÖ Traditional scraper: {len(traditional_methods)} public methods")
        logger.info(f"   ‚úÖ Intelligent scraper: {len(intelligent_methods)} public methods")
        
        # Agent
        from app.agents.scraper_agent import ScraperAgent
        agent = ScraperAgent("extended_diagnostic_agent")
        agent_status = agent.get_scraper_status()
        logger.info(f"   ‚úÖ Agent status: {agent_status.get('agent_name', 'unknown')}")
        
        # Base de donn√©es
        from app.models.database import get_db
        with next(get_db()) as db:
            # Test simple de connexion
            result = db.execute("SELECT 1").fetchone()
            logger.info(f"   ‚úÖ Database connection: result={result}")
        
        # Celery
        from app.celery_app import celery_app
        registered_tasks = list(celery_app.tasks.keys())
        important_tasks = [t for t in registered_tasks if 'app.' in t]
        logger.info(f"   ‚úÖ Celery: {len(registered_tasks)} total tasks, {len(important_tasks)} app tasks")
        
        # Redis
        import redis
        redis_url = os.getenv('REDIS_URL', 'redis://redis:6379/0')
        r = redis.from_url(redis_url)
        redis_info = r.info()
        logger.info(f"   ‚úÖ Redis: version={redis_info.get('redis_version', 'unknown')}")
        
        logger.info("üéâ EXTENDED DIAGNOSTICS COMPLETED SUCCESSFULLY")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå EXTENDED DIAGNOSTICS FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    # V√©rifier les arguments de ligne de commande
    if len(sys.argv) > 1:
        if sys.argv[1] == '--diagnose-only':
            # Mode diagnostic seulement
            success = run_diagnostics_only()
            sys.exit(0 if success else 1)
        elif sys.argv[1] == '--extended-diagnostics':
            # Mode diagnostic √©tendu
            success = run_extended_diagnostics()
            sys.exit(0 if success else 1)
        else:
            logger.warning(f"Unknown argument: {sys.argv[1]}")
            logger.info("Available options: --diagnose-only, --extended-diagnostics")
    
    # Mode normal - d√©marrer le worker
    sys.exit(main())