"""
Scraper Intelligent CORRIG√â - Int√©gration compl√®te des utils
Version coh√©rente utilisant tous les modules du projet
"""

import os
import re
import json
import requests
import time
from typing import Optional, Dict, List, Any
import logging
from datetime import datetime
from urllib.parse import urlparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# CORRECTION : Import des schemas
from app.models.schemas import (
    ScrapedContent, AnalysisResult, EconomicCategory,
    EnhancedExtractedValue, ExtractionSummary, SourceAnalysis, 
    ExtractionQuality, ProcessingInfo, SmartInsights, DataSummary,
    QualityAssessment, TemporalAnalysis, LLMAnalysis, 
    SettingsCompliance, ExtractionMethod, TemporalMetadata,
    ValidationDetails, EconomicCoherence
)

# CORRECTION : Import de la configuration
from app.config.settings import settings
from app.config.llm_config import analyze_with_fixed_llm, fixed_llm_config

# INT√âGRATION CRITIQUE : Import des utils
from app.utils.helpers import (
    format_timestamp, calculate_execution_time,
    extract_domain, generate_task_summary,
    detect_tunisian_content_patterns, suggest_optimal_strategy,
    debug_extraction_data, log_extraction_details, categorize_url_type
)
from app.utils.data_validator import validate_indicators_strict, is_economic_indicator_valid
from app.utils.temporal_filter import filter_by_temporal_period, is_in_target_period
from app.utils.clean_extraction_patterns import extract_clean_economic_data, is_valid_indicator
from app.utils.storage import smart_storage

# Import du scraper traditionnel comme base
from .traditional import TunisianWebScraper

logger = logging.getLogger(__name__)

def suggest_extraction_improvements(debug_info: Dict[str, Any]) -> List[str]:
    """Suggestions d'am√©lioration pour l'extraction"""
    suggestions = []
    
    if not debug_info:
        return ["Aucune information de debug disponible"]
    
    extraction_count = debug_info.get('extraction_count', 0)
    
    if extraction_count == 0:
        suggestions.append("Aucune donn√©e extraite - site peut n√©cessiter JavaScript")
        suggestions.append("Consid√©rer l'utilisation de Selenium")
    
    return suggestions if suggestions else ["Extraction optimale"]

class CohesiveIntelligentScraper(TunisianWebScraper):
    """Scraper Intelligent utilisant TOUS les modules du projet"""
    
    def __init__(self, delay: float = None):
        super().__init__(delay)
        
        # Configuration LLM coh√©rente avec les settings
        self.llm_available = settings.ENABLE_LLM_ANALYSIS
        self.ollama_url = settings.OLLAMA_HOST
        self.ollama_model = settings.OLLAMA_MODEL
        self.llm_timeout = settings.OLLAMA_TIMEOUT  # COH√âRENT avec settings
        
        # CRITICAL FIX: Initialize LLM config properly
        try:
            from app.config.llm_config import fixed_llm_config
            self.llm_config = fixed_llm_config
            logger.info(f"LLM config initialized - Available: {self.llm_config.llm_available}")
            # Update availability based on actual config
            if self.llm_available and not self.llm_config.llm_available:
                self.llm_available = False
                logger.warning("LLM config indicates service unavailable")
        except ImportError as e:
            logger.warning(f"Could not import LLM config: {e}")
            self.llm_config = None
            self.llm_available = False
        
        # Test de connectivit√© LLM automatique
        if self.llm_available:
            self.llm_available = self._test_llm_connection()
        
        # M√©triques d'utilisation des modules
        self.module_usage = {
            'utils_helpers': 0,
            'data_validator': 0,
            'temporal_filter': 0,
            'clean_extractor': 0,
            'storage': 0,
            'llm_analysis': 0
        }
        
        logger.info(f"CohesiveIntelligentScraper initialized - LLM: {'Available' if self.llm_available else 'Unavailable'}")
        logger.info(f"Configuration - Timeout: {self.llm_timeout}s, Model: {self.ollama_model}")

    def _test_llm_connection(self) -> bool:
        """Test automatique de connexion LLM avec configuration coh√©rente"""
        try:
            response = requests.get(
                f"{self.ollama_url}/api/tags", 
                timeout=settings.OLLAMA_CONNECTION_TIMEOUT
            )
            if response.status_code == 200:
                logger.info("LLM service available and configured")
                return True
            else:
                logger.warning(f"LLM service unavailable: HTTP {response.status_code}")
                return False
        except Exception as e:
            logger.warning(f"LLM connection test failed: {e}")
            return False

    def scrape_with_analysis(self, url: str, enable_llm_analysis: bool = False) -> Optional[ScrapedContent]:
        """Scraping intelligent optimis√© pour sites complexes tunisiens"""
        try:
            logger.info(f"üß† INTELLIGENT scraping for complex site: {url}")

            # √âTAPE 1 : D√©tection du type de site tunisien
            site_type = self._detect_tunisian_site_type(url)
            logger.info(f"Detected Tunisian site type: {site_type}")

            # √âTAPE 2 : Configuration adapt√©e au site
            scraping_config = self._get_site_specific_config(site_type, url)
            
            # √âTAPE 3 : Scraping avec strat√©gies multiples
            result = self._intelligent_multi_strategy_scraping(url, scraping_config)
            
            if not result:
                logger.warning(f"Multi-strategy scraping failed for {url}, using traditional fallback")
                # Fallback au scraping traditionnel
                result = super().scrape(url)
                if not result:
                    return self._create_fallback_result(url, "All strategies failed")

            # √âTAPE 4 : Post-processing sp√©cialis√©
            result = self._apply_tunisian_post_processing(result, site_type)

            # √âTAPE 5: CRITICAL FIX - LLM Enhancement with proper integration
            if enable_llm_analysis or site_type in ["government", "central_bank", "statistical_institute", "ministry_finance"]:
                logger.info("Applying LLM enhancement for complex site")
                result = self._enhance_with_llm_analysis_cohesive(result, url)
                
                # EXTRA CHECK: Ensure llm_analysis field exists in result
                if not hasattr(result, 'llm_analysis'):
                    result.llm_analysis = {}
                    logger.warning("Had to create missing llm_analysis attribute")
                
                # Ensure it's in structured_data for API response
                result.structured_data['llm_analysis'] = getattr(result, 'llm_analysis', {})

            logger.info(f"üéØ Intelligent scraping completed: {len(result.indicators)} indicators")
            return result

        except Exception as e:
            logger.error(f"Intelligent scraping failed for {url}: {e}")
            return self._create_fallback_result(url, str(e))

    def _apply_tunisian_post_processing(self, result: ScrapedContent, site_type: str) -> ScrapedContent:
        """Post-processing sp√©cialis√© pour les sites tunisiens"""
        try:
            # Appliquer le filtrage temporel
            result = self._apply_temporal_filtering(result)
            
            # Appliquer la validation stricte
            result = self._apply_strict_validation(result)
            
            # Appliquer l'extraction propre
            result = self._enhance_with_clean_extraction(result, result.metadata.get('url', ''))
            
            # Ajouter des insights intelligents
            tunisian_context = {
                'tunisian_context': True,
                'context_strength': 'high',
                'site_type': site_type
            }
            result = self._add_comprehensive_intelligent_insights(
                result, result.metadata.get('url', ''), tunisian_context
            )
            
            # CRITICAL FIX: Ensure LLM analysis field is initialized
            if not hasattr(result, 'llm_analysis'):
                result.llm_analysis = {}
            
            # Ensure it's available in structured_data for API response
            result.structured_data['llm_analysis'] = getattr(result, 'llm_analysis', {})
            
            logger.info(f"Tunisian post-processing completed for {site_type}")
            return result
            
        except Exception as e:
            logger.error(f"Tunisian post-processing failed: {e}")
            return result

    def _enhance_with_tunisian_llm_analysis(self, result: ScrapedContent, url: str, site_type: str) -> ScrapedContent:
        """Enhancement LLM sp√©cialis√© pour contexte tunisien"""
        
        try:
            # Prompt sp√©cialis√© selon le type de site
            context_prompts = {
                "statistical_institute": "Analyser les statistiques √©conomiques tunisiennes de l'INS",
                "central_bank": "Analyser les indicateurs mon√©taires de la Banque Centrale de Tunisie", 
                "ministry_finance": "Analyser les donn√©es budg√©taires du Minist√®re des Finances tunisien",
                "government": "Analyser les donn√©es √©conomiques gouvernementales tunisiennes"
            }
            
            prompt = f"""
            {context_prompts.get(site_type, "Analyser les donn√©es √©conomiques")}
            
            URL: {url}
            Donn√©es extraites: {len(result.indicators)} indicateurs
            
            Am√©liorer et enrichir ces donn√©es √©conomiques tunisiennes:
            - Identifier les indicateurs manqu√©s
            - Corriger les erreurs d'extraction
            - Ajouter le contexte √©conomique tunisien
            - Standardiser les unit√©s (TND, USD, %)
            """
            
            enhanced_result = self._call_llm_for_enhancement(prompt, result)
            return enhanced_result if enhanced_result else result
            
        except Exception as e:
            logger.error(f"LLM enhancement failed: {e}")
            return result

    def _call_llm_for_enhancement(self, prompt: str, result: ScrapedContent) -> Optional[ScrapedContent]:
        """Appel LLM pour l'enhancement des donn√©es"""
        try:
            if not self.llm_available:
                logger.warning("LLM not available for enhancement")
                return result
            
            # Pr√©parer les donn√©es pour l'analyse LLM
            extracted_values = result.structured_data.get('extracted_values', {})
            values_summary = self._create_values_summary_for_llm(extracted_values)
            
            # Utiliser le service LLM configur√©
            llm_result = analyze_with_fixed_llm(
                content=values_summary,
                context="economic_enhancement",
                source_domain=extract_domain(result.metadata.get('url', ''))
            )
            
            if llm_result and llm_result.get('success'):
                # Int√©grer les r√©sultats LLM
                result.metadata['llm_enhancement'] = {
                    'status': 'completed',
                    'analysis': llm_result['analysis'],
                    'execution_time': llm_result.get('execution_time', 0),
                    'timestamp': datetime.utcnow().isoformat()
                }
                logger.info("LLM enhancement completed successfully")
            
            return result
            
        except Exception as e:
            logger.error(f"LLM enhancement call failed: {e}")
            return result

    def _detect_tunisian_site_type(self, url: str) -> str:
        """D√©tection du type de site tunisien pour strat√©gie adapt√©e"""
        url_lower = url.lower()
        
        if 'ins.tn' in url_lower:
            return "statistical_institute"
        elif 'bct.gov.tn' in url_lower:
            return "central_bank" 
        elif 'finances.gov.tn' in url_lower:
            return "ministry_finance"
        elif 'tunisieindustrie' in url_lower:
            return "industry_portal"
        elif '.gov.tn' in url_lower:
            return "government"
        else:
            return "complex_site"
            
    def _get_site_specific_config(self, site_type: str, url: str) -> Dict[str, Any]:
        """Configuration sp√©cialis√©e par type de site"""
        
        base_config = {
            "timeout": 90,  # Plus long pour sites gouvernementaux
            "retries": 5,   # Plus de tentatives
            "use_session": True,
            "javascript_wait": False,
            "headers": {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'fr-TN,fr;q=0.9,ar;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0'
            }
        }
        
        site_configs = {
            "statistical_institute": {
                **base_config,
                "timeout": 120,  # INS peut √™tre tr√®s lent
                "patterns": [
                    r'indicateur.*\d+',
                    r'statistique.*\d+', 
                    r'donn√©es.*\d+',
                    r'taux.*\d+[.,]\d*',
                    r'population.*\d+',
                    r'emploi.*\d+',
                    r'ch√¥mage.*\d+[.,]\d*'
                ],
                "selectors": [
                    'table.data-table td',
                    '.statistique-value',
                    '.indicator-row',
                    '.tableau-statistique tr',
                    '.valeur-indicateur',
                    '.donnee-chiffree'
                ],
                "javascript_wait": True
            },
            
            "central_bank": {
                **base_config,
                "timeout": 90,
                "patterns": [
                    r'taux.*change.*\d+',
                    r'r√©serves.*\d+',
                    r'balance.*\d+',
                    r'cr√©dit.*\d+[.,]\d*',
                    r'inflation.*\d+[.,]\d*',
                    r'tmm.*\d+[.,]\d*'
                ],
                "selectors": [
                    'table tr td',
                    '.tableau-statistique td',
                    '.indicator-cell',
                    '.valeur',
                    '.data-cell'
                ]
            },
            
            "ministry_finance": {
                **base_config,
                "patterns": [
                    r'budget.*\d+',
                    r'd√©ficit.*\d+',
                    r'recettes.*\d+',
                    r'd√©penses.*\d+[.,]\d*',
                    r'dette.*\d+',
                    r'pib.*\d+[.,]\d*'
                ],
                "selectors": [
                    '.financial-data td',
                    '.budget-table td',
                    '.economic-indicator',
                    '.valeur-budgetaire',
                    '.donnee-financiere'
                ]
            }
        }
        
        return site_configs.get(site_type, base_config)

    def _intelligent_multi_strategy_scraping(self, url: str, config: Dict) -> Optional[ScrapedContent]:
        """Scraping multi-strat√©gies pour sites complexes avec fallback"""
        
        strategies = [
            self._strategy_enhanced_requests,
            self._strategy_session_based,
            self._strategy_form_submission,
            self._strategy_ajax_simulation
        ]
        
        best_result = None
        max_indicators = 0
        
        for i, strategy in enumerate(strategies):
            try:
                logger.info(f"Trying strategy {i+1}/{len(strategies)}: {strategy.__name__}")
                result = strategy(url, config)
                
                if result and result.indicators:
                    indicator_count = len(result.indicators)
                    logger.info(f"Strategy {strategy.__name__} succeeded with {indicator_count} indicators")
                    
                    # Garder le meilleur r√©sultat
                    if indicator_count > max_indicators:
                        best_result = result
                        max_indicators = indicator_count
                        
            except Exception as e:
                logger.warning(f"Strategy {strategy.__name__} failed: {e}")
                continue
        
        # Si aucune strat√©gie intelligente ne fonctionne, essayer le scraping traditionnel
        if not best_result:
            logger.info("All intelligent strategies failed, trying traditional fallback")
            try:
                traditional_result = super().scrape(url)
                if traditional_result and traditional_result.indicators:
                    return traditional_result
            except Exception as e:
                logger.error(f"Traditional fallback also failed: {e}")
        
        return best_result
        
    def _strategy_form_submission(self, url: str, config: Dict) -> Optional[ScrapedContent]:
        """Strat√©gie de soumission de formulaires pour sites complexes"""
        logger.info(f"Attempting form submission strategy for {url}")
        
        try:
            session = requests.Session()
            response = session.get(url, timeout=config.get('timeout', 60))
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            forms = soup.find_all('form')
            
            if not forms:
                logger.debug("No forms found on page")
                return None
            
            extracted_data = {}
            form_count = 0
            
            for form in forms[:3]:  # Limiter √† 3 formulaires max
                try:
                    form_action = form.get('action', '')
                    form_method = form.get('method', 'get').lower()
                    
                    # Construire les donn√©es du formulaire
                    form_data = {}
                    inputs = form.find_all(['input', 'select', 'textarea'])
                    
                    for input_elem in inputs:
                        name = input_elem.get('name')
                        if name:
                            input_type = input_elem.get('type', 'text')
                            if input_type in ['text', 'hidden', 'number']:
                                value = input_elem.get('value', '')
                                form_data[name] = value
                            elif input_type == 'submit':
                                form_data[name] = input_elem.get('value', 'Submit')
                    
                    if form_data:
                        # Construire l'URL de soumission
                        if form_action.startswith('http'):
                            submit_url = form_action
                        elif form_action.startswith('/'):
                            submit_url = f"{url.split('/')[0]}//{url.split('/')[2]}{form_action}"
                        else:
                            submit_url = f"{url.rstrip('/')}/{form_action}"
                        
                        # Soumettre le formulaire
                        if form_method == 'post':
                            response = session.post(submit_url, data=form_data, timeout=30)
                        else:
                            response = session.get(submit_url, params=form_data, timeout=30)
                        
                        if response.status_code == 200:
                            form_soup = BeautifulSoup(response.text, 'html.parser')
                            form_data_extracted = self._extract_from_soup(form_soup, url)
                            
                            if form_data_extracted:
                                extracted_data[f'form_{form_count}'] = form_data_extracted
                                form_count += 1
                                logger.info(f"Form submission successful, extracted {len(form_data_extracted)} items")
                    
                except Exception as e:
                    logger.debug(f"Form submission failed: {e}")
                    continue
            
            if extracted_data:
                return ScrapedContent(
                    raw_content=response.text,
                    structured_data={'extracted_values': extracted_data},
                    metadata={'extraction_method': 'form_submission'}
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Form submission strategy failed: {e}")
            return None

    def _strategy_ajax_simulation(self, url: str, config: Dict) -> Optional[ScrapedContent]:
        """Strat√©gie de simulation d'appels AJAX"""
        logger.info(f"Attempting AJAX simulation strategy for {url}")
        
        try:
            session = requests.Session()
            response = session.get(url, timeout=config.get('timeout', 60))
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            extracted_data = {}
            
            # Chercher des patterns d'URLs AJAX dans le HTML
            ajax_patterns = [
                r'url\s*:\s*["\']([^"\']*\.json)["\']',
                r'ajax\s*:\s*["\']([^"\']*)["\']',
                r'data-url=["\']([^"\']*)["\']',
                r'api["\']([^"\']*)["\']'
            ]
            
            html_content = str(soup)
            ajax_urls = set()
            
            for pattern in ajax_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    if match.startswith('http'):
                        ajax_urls.add(match)
                    elif match.startswith('/'):
                        ajax_urls.add(f"{url.split('/')[0]}//{url.split('/')[2]}{match}")
                    else:
                        ajax_urls.add(f"{url.rstrip('/')}/{match}")
            
            # Essayer les URLs AJAX trouv√©es
            for ajax_url in list(ajax_urls)[:5]:  # Limiter √† 5 URLs
                try:
                    headers = {
                        'X-Requested-With': 'XMLHttpRequest',
                        'Accept': 'application/json, text/javascript, */*; q=0.01',
                        'Referer': url
                    }
                    
                    response = session.get(ajax_url, headers=headers, timeout=30)
                    
                    if response.status_code == 200:
                        # Essayer de parser comme JSON
                        try:
                            json_data = response.json()
                            if isinstance(json_data, dict) and json_data:
                                ajax_extracted = self._extract_from_json_response(json_data, ajax_url)
                                if ajax_extracted:
                                    extracted_data[f'ajax_{len(extracted_data)}'] = ajax_extracted
                                    logger.info(f"AJAX call successful: {ajax_url}")
                        except:
                            # Si ce n'est pas du JSON, parser comme HTML
                            ajax_soup = BeautifulSoup(response.text, 'html.parser')
                            ajax_extracted = self._extract_from_soup(ajax_soup, ajax_url)
                            if ajax_extracted:
                                extracted_data[f'ajax_{len(extracted_data)}'] = ajax_extracted
                    
                except Exception as e:
                    logger.debug(f"AJAX call failed for {ajax_url}: {e}")
                    continue
            
            if extracted_data:
                return ScrapedContent(
                    raw_content=response.text,
                    structured_data={'extracted_values': extracted_data},
                    metadata={'extraction_method': 'ajax_simulation'}
                )
            
            return None
            
        except Exception as e:
            logger.error(f"AJAX simulation strategy failed: {e}")
            return None
        
    def _extract_from_soup(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """Extraction de base depuis BeautifulSoup"""
        extracted = {}
        
        try:
            # Extraction depuis les tables
            tables = soup.find_all('table')[:5]
            for table_idx, table in enumerate(tables):
                rows = table.find_all('tr')
                for row_idx, row in enumerate(rows):
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text(strip=True)
                        value_text = cells[1].get_text(strip=True)
                        
                        # Essayer d'extraire une valeur num√©rique
                        numeric_match = re.search(r'([0-9]+[,.]?[0-9]*)', value_text)
                        if numeric_match and len(label) > 3:
                            try:
                                value = float(numeric_match.group(1).replace(',', '.'))
                                key = f"table_{table_idx}_{row_idx}"
                                extracted[key] = {
                                    'value': value,
                                    'indicator_name': label[:60],
                                    'raw_text': f"{label}: {value_text}",
                                    'extraction_method': 'table_extraction',
                                    'source': url
                                }
                            except ValueError:
                                continue
            
            return extracted
            
        except Exception as e:
            logger.error(f"Soup extraction failed: {e}")
            return {}
        
    def _extract_from_json_response(self, json_data: dict, source_url: str) -> Dict[str, Any]:
        """Extraction depuis une r√©ponse JSON"""
        extracted = {}
        
        def extract_recursive(data, prefix=""):
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, (int, float)) and key.lower() not in ['id', 'code', 'status']:
                        # Potentielle valeur √©conomique
                        item_key = f"{prefix}_{key}" if prefix else key
                        extracted[item_key] = {
                            'value': value,
                            'indicator_name': key,
                            'source': source_url,
                            'extraction_method': 'ajax_json'
                        }
                    elif isinstance(value, (dict, list)):
                        new_prefix = f"{prefix}_{key}" if prefix else key
                        extract_recursive(value, new_prefix)
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    if isinstance(item, (dict, list)):
                        extract_recursive(item, f"{prefix}_{i}" if prefix else str(i))
        
        extract_recursive(json_data)
        return extracted
        
    def _strategy_enhanced_requests(self, url: str, config: Dict) -> Optional[ScrapedContent]:
        """Strat√©gie 1: Requ√™tes HTTP am√©lior√©es avec headers tunisiens"""
        
        session = requests.Session()
        
        # Headers sp√©cialis√©s pour sites tunisiens
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-TN,fr;q=0.9,ar-TN;q=0.8,ar;q=0.7,en;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = session.get(url, headers=headers, timeout=config['timeout'])
        
        if response.status_code == 200:
            return self._extract_with_patterns(response.text, url, config['patterns'])
        
        return None
        
    def _extract_with_patterns(self, html_content: str, url: str, patterns: List[str]) -> Optional[ScrapedContent]:
        """Extraction utilisant des patterns regex"""
        try:
            extracted_data = {}
            
            for i, pattern in enumerate(patterns):
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        match = ' '.join([m for m in match if m])
                    
                    # Essayer d'extraire une valeur num√©rique
                    numeric_match = re.search(r'([0-9]+[,.]?[0-9]*)', match)
                    if numeric_match:
                        try:
                            value = float(numeric_match.group(1).replace(',', '.'))
                            key = f"pattern_{i}_{len(extracted_data)}"
                            extracted_data[key] = {
                                'value': value,
                                'indicator_name': f"Pattern_{i}",
                                'raw_text': match,
                                'extraction_method': 'pattern_regex',
                                'source': url
                            }
                        except ValueError:
                            continue
            
            if extracted_data:
                return ScrapedContent(
                    raw_content=html_content,
                    structured_data={'extracted_values': extracted_data},
                    metadata={'extraction_method': 'pattern_based'}
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Pattern extraction failed: {e}")
            return None    


    def _strategy_session_based(self, url: str, config: Dict) -> Optional[ScrapedContent]:
        """Strat√©gie 2: Session avec cookies pour sites gouvernementaux"""
        
        session = requests.Session()
        
        # Simuler navigation normale avec landing page
        base_url = '/'.join(url.split('/')[:3])
        
        try:
            # Page d'accueil pour cookies
            session.get(base_url, timeout=15)
            
            # Page cible avec session √©tablie
            response = session.get(url, timeout=config['timeout'])
            
            if response.status_code == 200:
                return self._extract_with_selectors(response.text, url, config['selectors'])
                
        except Exception as e:
            logger.warning(f"Session strategy failed: {e}")
        
        return None
        
    def _extract_with_selectors(self, html_content: str, url: str, selectors: List[str]) -> Optional[ScrapedContent]:
        """Extraction utilisant des s√©lecteurs CSS"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            extracted_data = {}
            
            for selector in selectors:
                try:
                    elements = soup.select(selector)
                    for elem in elements[:20]:  # Limiter pour performance
                        text = elem.get_text(strip=True)
                        if text and len(text) > 3:
                            # Chercher des valeurs num√©riques
                            numeric_match = re.search(r'([0-9]+[,.]?[0-9]*)', text)
                            if numeric_match:
                                try:
                                    value = float(numeric_match.group(1).replace(',', '.'))
                                    key = f"selector_{selector}_{len(extracted_data)}"
                                    extracted_data[key] = {
                                        'value': value,
                                        'indicator_name': f"Selector_{selector}",
                                        'raw_text': text,
                                        'extraction_method': 'css_selector',
                                        'source': url
                                    }
                                except ValueError:
                                    continue
                except Exception as e:
                    logger.debug(f"Selector {selector} failed: {e}")
                    continue
            
            if extracted_data:
                return ScrapedContent(
                    raw_content=html_content,
                    structured_data={'extracted_values': extracted_data},
                    metadata={'extraction_method': 'selector_based'}
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Selector extraction failed: {e}")
            return None

    def _create_fallback_result(self, url: str, error_message: str) -> ScrapedContent:
        """Cr√©e un r√©sultat de fallback pour √©viter les √©checs complets - FIXED VERSION"""
        
        fallback = ScrapedContent(
            raw_content="",
            structured_data={
                'extracted_values': {},
                'llm_analysis': {},  # CRITICAL FIX: Add this line
                'fallback_mode': True,
                'error_recovery': True,
                'original_error': error_message
            },
            metadata={
                'scraping_method': 'fallback_recovery',
                'error_message': error_message,
                'timestamp': datetime.utcnow().isoformat(),
                'fallback_reason': 'intelligent_scraper_error_recovery',
                'url': url,
                'llm_enhancement': {
                    'status': 'skipped',
                    'analysis': {},
                    'reason': 'fallback_mode'
                }
            }
        )
        
        # CRITICAL FIX: Ensure llm_analysis attribute exists
        fallback.llm_analysis = {}
        return fallback
        
    def _ensure_llm_analysis_structure(self, result: ScrapedContent) -> ScrapedContent:
        """Ensure LLM analysis structure is always present"""
        
        # Ensure attribute exists
        if not hasattr(result, 'llm_analysis'):
            result.llm_analysis = {}
        
        # Ensure it's in structured_data
        if 'llm_analysis' not in result.structured_data:
            result.structured_data['llm_analysis'] = getattr(result, 'llm_analysis', {})
        
        # Ensure metadata has llm_enhancement
        if 'llm_enhancement' not in result.metadata:
            result.metadata['llm_enhancement'] = {
                'status': 'not_applied',
                'analysis': {}
            }
        
        return result

    def _enhance_with_clean_extraction(self, result: ScrapedContent, url: str) -> ScrapedContent:
        """INT√âGRATION clean_extraction_patterns dans le pipeline"""
        
        try:
            self.module_usage['clean_extractor'] += 1
            
            # Extraction propre en compl√©ment
            clean_values = extract_clean_economic_data(result.raw_content, url)
            
            if clean_values:
                # Fusionner avec les valeurs existantes
                existing_values = result.structured_data.get('extracted_values', {})
                
                # Ajouter les nouvelles valeurs avec pr√©fixe
                for key, value in clean_values.items():
                    clean_key = f"clean_{key}"
                    existing_values[clean_key] = value
                
                result.structured_data['extracted_values'] = existing_values
                
                # M√©tadonn√©es de l'extraction propre
                result.structured_data['clean_extraction'] = {
                    'applied': True,
                    'values_added': len(clean_values),
                    'total_values': len(existing_values),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                logger.info(f"Clean extraction added {len(clean_values)} values")
            
            return result
            
        except Exception as e:
            logger.error(f"Clean extraction enhancement failed: {e}")
            return result
    
    def _apply_temporal_filtering(self, result: ScrapedContent) -> ScrapedContent:
        """INT√âGRATION temporal_filter dans le pipeline"""
        
        try:
            self.module_usage['temporal_filter'] += 1
            
            extracted_values = result.structured_data.get('extracted_values', {})
            
            if extracted_values:
                # Appliquer le filtrage temporel
                filtered_values = filter_by_temporal_period(extracted_values, result.raw_content or "")
                
                # M√©tadonn√©es du filtrage
                result.structured_data['temporal_filtering'] = {
                    'applied': True,
                    'original_count': len(extracted_values),
                    'filtered_count': len(filtered_values),
                    'filtered_out': len(extracted_values) - len(filtered_values),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                # Remplacer les valeurs
                result.structured_data['extracted_values'] = filtered_values
                
                logger.info(f"Temporal filtering: {len(extracted_values)} -> {len(filtered_values)} values")
            
            return result
            
        except Exception as e:
            logger.error(f"Temporal filtering failed: {e}")
            return result
    
    def _apply_strict_validation(self, result: ScrapedContent) -> ScrapedContent:
        """INT√âGRATION data_validator dans le pipeline"""
        
        try:
            self.module_usage['data_validator'] += 1
            
            extracted_values = result.structured_data.get('extracted_values', {})
            
            if extracted_values:
                # Convertir en format attendu par le validateur
                values_for_validation = [
                    v for v in extracted_values.values() 
                    if isinstance(v, dict)
                ]
                
                # Validation stricte
                validation_result = validate_indicators_strict(
                    values_for_validation, 
                    content=result.raw_content or ""
                )
                
                # Traitement du r√©sultat de validation
                if validation_result['valid'] and validation_result['valid_data']:
                    # Remplacer par les donn√©es valid√©es
                    validated_dict = {
                        f"validated_{i}": item 
                        for i, item in enumerate(validation_result['valid_data'])
                    }
                    
                    result.structured_data['extracted_values'] = validated_dict
                    result.structured_data['strict_validation'] = {
                        'applied': True,
                        'validation_summary': validation_result['summary'],
                        'errors': validation_result.get('errors', []),
                        'timestamp': datetime.utcnow().isoformat()
                    }
                    
                    logger.info(f"Strict validation: {validation_result['summary']['valid_output']} valid indicators")
                else:
                    logger.warning("Strict validation failed, keeping original data")
                    result.structured_data['strict_validation'] = {
                        'applied': False,
                        'reason': 'validation_failed',
                        'errors': validation_result.get('errors', [])
                    }
            
            return result
            
        except Exception as e:
            logger.error(f"Strict validation failed: {e}")
            return result

    def _should_activate_llm_cohesive(self, url: str, user_requested: bool, 
                                   base_result: ScrapedContent, tunisian_context: Dict[str, Any]) -> bool:
        """D√©cision LLM coh√©rente avec l'architecture int√©gr√©e"""
        
        # Toujours respecter la demande utilisateur
        if user_requested:
            return self.llm_available
        
        # Si pas disponible, pas d'activation
        if not self.llm_available:
            return False
        
        # Activation intelligente bas√©e sur le contexte tunisien
        if tunisian_context.get('tunisian_context', False):
            strength = tunisian_context.get('context_strength', 'none')
            if strength in ['high', 'medium']:
                logger.info("LLM activated for strong Tunisian context")
                return True
        
        # Activation bas√©e sur le contenu extrait
        extracted_values = base_result.structured_data.get('extracted_values', {})
        if len(extracted_values) < 5:  # Peu de donn√©es extraites
            logger.info("LLM activated for low extraction count")
            return True
        
        # Activation pour sites complexes
        if any(site in url.lower() for site in ['bct.gov.tn', 'ins.tn']):
            logger.info("LLM activated for complex government site")
            return True
        
        # Par d√©faut : d√©sactiv√© pour la performance
        return False

    def _enhance_with_llm_analysis_cohesive(self, scraped_content: ScrapedContent, url: str) -> ScrapedContent:
        """Enhancement LLM utilisant la configuration coh√©rente - FIXED VERSION"""
        
        try:
            self.module_usage['llm_analysis'] += 1
            
            if not self.llm_available or not self.llm_config:
                logger.warning("LLM not available for enhancement")
                # Ensure empty LLM analysis structure
                scraped_content.llm_analysis = {}
                scraped_content.structured_data['llm_analysis'] = {}
                return scraped_content
            
            # Pr√©paration du contenu pour analyse LLM
            extracted_values = scraped_content.structured_data.get('extracted_values', {})
            
            # Utiliser analyze_with_fixed_llm pour la coh√©rence
            if extracted_values:
                # Cr√©er un r√©sum√© des valeurs extraites
                values_summary = self._create_values_summary_for_llm(extracted_values)
                llm_result = analyze_with_fixed_llm(
                    content=values_summary,
                    context="economic",
                    source_domain=extract_domain(url)
                )
            else:
                # Analyser le contenu brut
                content_sample = scraped_content.raw_content[:3000] if scraped_content.raw_content else ""
                llm_result = analyze_with_fixed_llm(
                    content=content_sample,
                    context="economic",
                    source_domain=extract_domain(url)
                )
            
            # CRITICAL FIX: Proper LLM result integration
            if llm_result and llm_result.get('success'):
                llm_analysis_data = llm_result.get('analysis', {})
                
                # Store in the main result structure (this is what the API returns)
                scraped_content.llm_analysis = llm_analysis_data
                scraped_content.structured_data['llm_analysis'] = llm_analysis_data
                
                # Also store in metadata for backward compatibility
                scraped_content.metadata['llm_enhancement'] = {
                    'status': 'completed',
                    'analysis': llm_analysis_data,
                    'execution_time': llm_result.get('execution_time', 0),
                    'timeout_type': llm_result.get('timeout_type', 'standard'),
                    'method': llm_result.get('method', 'fixed_llm_config'),
                    'timestamp': datetime.utcnow().isoformat()
                }
                
                logger.info(f"LLM enhancement completed successfully for {url}")
                logger.info(f"LLM found {len(llm_analysis_data.get('indicateurs', []))} indicators")
                
            else:
                logger.warning("LLM enhancement failed, using fallback")
                # Ensure empty but present structure
                scraped_content.llm_analysis = {}
                scraped_content.structured_data['llm_analysis'] = {}
                scraped_content.metadata['llm_enhancement'] = {
                    'status': 'failed',
                    'analysis': {},
                    'fallback_reason': llm_result.get('fallback_reason', 'Unknown') if llm_result else 'No LLM result',
                    'timestamp': datetime.utcnow().isoformat()
                }
            
            return scraped_content
            
        except Exception as e:
            logger.error(f"LLM enhancement error: {e}")
            # Ensure structure exists even on error
            scraped_content.llm_analysis = {}
            scraped_content.structured_data['llm_analysis'] = {}
            scraped_content.metadata['llm_enhancement'] = {
                'status': 'error',
                'analysis': {},
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
            return scraped_content

    def _create_values_summary_for_llm(self, extracted_values: Dict[str, Any]) -> str:
        """Cr√©e un r√©sum√© des valeurs pour l'analyse LLM"""
        
        if not extracted_values:
            return "Aucune valeur extraite"
        
        summary_parts = []
        
        for i, (key, value_data) in enumerate(extracted_values.items()):
            if i >= 10:  # Limiter √† 10 valeurs pour √©viter les timeouts
                break
            
            if isinstance(value_data, dict):
                indicator = value_data.get('indicator_name', 'Inconnu')
                value = value_data.get('value', 'N/A')
                unit = value_data.get('unit', '')
                year = value_data.get('year', 'N/A')
                
                summary_parts.append(f"{indicator}: {value} {unit} ({year})")
        
        return "Donn√©es √©conomiques tunisiennes extraites:\n" + "\n".join(summary_parts)

    def _add_comprehensive_intelligent_insights(self, scraped_content: ScrapedContent, url: str, 
                                              tunisian_context: Dict[str, Any]) -> ScrapedContent:
        """Ajout d'insights intelligents utilisant TOUS les modules - FIXED VERSION"""
        
        try:
            extracted_values = scraped_content.structured_data.get('extracted_values', {})
            
            # Insights avec debug_extraction_data de helpers
            debug_info = debug_extraction_data(scraped_content.structured_data, url)
            
            # Log d√©taill√© avec log_extraction_details de helpers
            log_extraction_details(scraped_content.structured_data, url, "cohesive_intelligent")
            
            # Suggestions d'am√©lioration
            improvements = suggest_extraction_improvements(debug_info)
            
            # G√©n√©ration du r√©sum√© de t√¢che avec helpers
            task_summary = generate_task_summary({
                'urls': [url],
                'results': [scraped_content.structured_data],
                'status': 'completed',
                'metadata': scraped_content.metadata
            })
            
            # CRITICAL FIX: Ensure LLM analysis structure before adding insights
            scraped_content = self._ensure_llm_analysis_structure(scraped_content)
            
            # Enrichissement complet des m√©tadonn√©es
            if not scraped_content.metadata:
                scraped_content.metadata = {}
            
            scraped_content.metadata.update({
                'cohesive_intelligence': {
                    'modules_used': {
                        'helpers': True,
                        'data_validator': self.module_usage['data_validator'] > 0,
                        'temporal_filter': self.module_usage['temporal_filter'] > 0,
                        'clean_extractor': self.module_usage['clean_extractor'] > 0,
                        'storage': self.module_usage['storage'] > 0,
                        'llm_analysis': self.module_usage['llm_analysis'] > 0
                    },
                    'usage_stats': self.module_usage,
                    'debug_info': debug_info,
                    'task_summary': task_summary,
                    'improvement_suggestions': improvements,
                    'tunisian_context': tunisian_context,
                    'cohesion_timestamp': datetime.utcnow().isoformat()
                }
            })
            
            # Ajout aux donn√©es structur√©es
            scraped_content.structured_data.update({
                'cohesive_insights': {
                    'comprehensive_analysis': True,
                    'all_utils_integrated': True,
                    'debug_analysis': debug_info,
                    'task_summary': task_summary,
                    'improvements': improvements
                }
            })
            
            logger.info("Comprehensive intelligent insights added successfully")
            return scraped_content
            
        except Exception as e:
            logger.error(f"Failed to add comprehensive insights: {e}")
            return scraped_content

    def _save_via_integrated_storage(self, result: ScrapedContent, url: str):
        """Sauvegarde via le syst√®me de stockage int√©gr√©"""
        
        try:
            self.module_usage['storage'] += 1
            
            # Pr√©paration des donn√©es pour sauvegarde
            save_data = {
                'url': url,
                'content': result.raw_content,
                'structured_data': result.structured_data,
                'metadata': result.metadata,
                'cohesive_scraper_info': {
                    'scraper_type': 'CohesiveIntelligentScraper',
                    'modules_used': list(self.module_usage.keys()),
                    'usage_stats': self.module_usage,
                    'timestamp': datetime.utcnow().isoformat()
                }
            }
            
            # Sauvegarde via smart_storage
            storage_result = smart_storage.save_scraping_result(save_data)
            
            if storage_result['success']:
                logger.info(f"Data saved via integrated storage: {storage_result['storage_methods']}")
                
                # Enrichir les m√©tadonn√©es avec les infos de sauvegarde
                if not result.metadata:
                    result.metadata = {}
                
                result.metadata['storage_info'] = {
                    'saved_via': 'integrated_smart_storage',
                    'storage_methods': storage_result['storage_methods'],
                    'storage_timestamp': storage_result['timestamp']
                }
            else:
                logger.warning(f"Integrated storage failed: {storage_result['errors']}")
                
        except Exception as e:
            logger.error(f"Integrated storage error: {e}")

    def _validate_url_with_helpers(self, url: str) -> bool:
        """Validation URL utilisant les helpers int√©gr√©s - CORRIG√âE"""
        
        try:
            # CORRECTION : Utiliser une validation URL simple au lieu d'importer validate_url
            parsed = urlparse(url)
            
            # Validation basique
            if not parsed.scheme or not parsed.netloc:
                logger.error(f"URL validation failed: {url}")
                return False
            
            # Utiliser les fonctions helpers qui existent r√©ellement
            domain = extract_domain(url)
            url_category = categorize_url_type(url)
            
            logger.debug(f"URL validation: {url} -> Valid (domain: {domain})")
            return True
                    
        except Exception as e:
            logger.error(f"URL validation error: {e}")
            return False

    def get_cohesive_scraper_info(self) -> Dict[str, Any]:
        """Informations compl√®tes du scraper coh√©sif"""
        
        base_info = super().get_scraper_info()
        
        cohesive_info = {
            'scraper_type': 'CohesiveIntelligentScraper',
            'version': '2.0_fully_integrated',
            'parent_scraper': 'TunisianWebScraper',
            'llm_available': self.llm_available,
            'llm_config': {
                'model': self.ollama_model,
                'timeout': self.llm_timeout,
                'url': self.ollama_url
            },
            'module_integrations': {
                'helpers': 'fully_integrated',
                'data_validator': 'integrated_with_pipeline',
                'temporal_filter': 'integrated_with_pipeline', 
                'clean_extractor': 'integrated_with_pipeline',
                'storage': 'integrated_automatic',
                'llm_config': 'cohesive_timeouts'
            },
            'usage_statistics': self.module_usage,
            'intelligent_features': [
                'automatic_llm_decision',
                'comprehensive_post_processing',
                'integrated_validation_pipeline',
                'smart_storage_automatic',
                'tunisian_context_detection',
                'cohesive_module_integration',
                'debug_and_improvement_tracking'
            ],
            'cohesion_metadata': {
                'all_utils_integrated': True,
                'no_module_left_behind': True,
                'comprehensive_pipeline': True,
                'intelligent_coordination': True
            }
        }
        
        base_info.update(cohesive_info)
        return base_info
        
    

    def health_check(self) -> Dict[str, Any]:
        """V√©rification de sant√© du scraper coh√©sif"""
        
        base_health = super().health_check()
        
        cohesive_health = {
            'cohesive_features': {
                'utils_integration': 'operational',
                'llm_integration': 'available' if self.llm_available else 'unavailable',
                'storage_integration': 'operational',
                'validation_pipeline': 'operational'
            },
            'module_status': {
                name: 'used' if count > 0 else 'available'
                for name, count in self.module_usage.items()
            },
            'integration_score': sum(1 for count in self.module_usage.values() if count > 0) / len(self.module_usage),
            'cohesion_status': 'fully_integrated'
        }
        
        base_health.update(cohesive_health)
        return base_health

# Alias pour compatibilit√© avec le reste du syst√®me
IntelligentScraper = CohesiveIntelligentScraper