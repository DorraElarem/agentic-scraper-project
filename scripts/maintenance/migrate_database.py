#!/usr/bin/env python3
"""
Script de migration pour ajouter la colonne progress et corriger la structure de la base de donn√©es
"""

import os
import sys
import psycopg2
from sqlalchemy import create_engine, text, inspect
import json
import logging

# Configuration des logs
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_database_url():
    """G√©n√®re l'URL de connexion √† la base de donn√©es"""
    db_user = os.getenv("POSTGRES_USER", "postgres")
    db_pass = os.getenv("POSTGRES_PASSWORD", "dorra123")
    db_host = os.getenv("DB_HOST", "db")
    db_name = os.getenv("POSTGRES_DB", "scraper_db")
    return f"postgresql://{db_user}:{db_pass}@{db_host}:5432/{db_name}"

def check_and_add_column(engine, table_name, column_name, column_type, default_value=None):
    """V√©rifie et ajoute une colonne si elle n'existe pas"""
    try:
        inspector = inspect(engine)
        columns = [col['name'] for col in inspector.get_columns(table_name)]
        
        if column_name not in columns:
            logger.info(f"Ajout de la colonne '{column_name}' √† la table '{table_name}'...")
            
            with engine.connect() as conn:
                trans = conn.begin()
                try:
                    if default_value:
                        sql = f"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_type} DEFAULT '{default_value}'"
                    else:
                        sql = f"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_type}"
                    
                    conn.execute(text(sql))
                    trans.commit()
                    logger.info(f"‚úÖ Colonne '{column_name}' ajout√©e avec succ√®s")
                    return True
                except Exception as e:
                    trans.rollback()
                    logger.error(f"‚ùå Erreur lors de l'ajout de la colonne: {e}")
                    return False
        else:
            logger.info(f"‚úÖ Colonne '{column_name}' existe d√©j√†")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification de la colonne: {e}")
        return False

def migrate_progress_data(engine):
    """Migre les donn√©es de progress_current/progress_total vers progress JSON"""
    try:
        inspector = inspect(engine)
        columns = [col['name'] for col in inspector.get_columns('scraping_tasks')]
        
        # Si les anciennes colonnes existent, migrer les donn√©es
        if 'progress_current' in columns and 'progress_total' in columns:
            logger.info("Migration des donn√©es progress...")
            
            with engine.connect() as conn:
                trans = conn.begin()
                try:
                    # Migrer les donn√©es existantes
                    result = conn.execute(text("""
                        SELECT id, progress_current, progress_total 
                        FROM scraping_tasks 
                        WHERE progress_current IS NOT NULL OR progress_total IS NOT NULL
                    """))
                    
                    rows_migrated = 0
                    for row in result:
                        current = int(row.progress_current or 0)
                        total = int(row.progress_total or 1)
                        if total <= 0:
                            total = 1
                        if current > total:
                            current = total
                        
                        percentage = round((current / total) * 100, 2)
                        progress_json = {
                            "current": current,
                            "total": total,
                            "percentage": percentage,
                            "display": f"{current}/{total}"
                        }
                        
                        conn.execute(text("""
                            UPDATE scraping_tasks 
                            SET progress = :progress_data 
                            WHERE id = :row_id
                        """), {
                            "progress_data": json.dumps(progress_json),
                            "row_id": row.id
                        })
                        rows_migrated += 1
                    
                    # Supprimer les anciennes colonnes
                    conn.execute(text("ALTER TABLE scraping_tasks DROP COLUMN IF EXISTS progress_current"))
                    conn.execute(text("ALTER TABLE scraping_tasks DROP COLUMN IF EXISTS progress_total"))
                    
                    trans.commit()
                    logger.info(f"‚úÖ Migration termin√©e: {rows_migrated} lignes migr√©es")
                    return True
                    
                except Exception as e:
                    trans.rollback()
                    logger.error(f"‚ùå Erreur lors de la migration: {e}")
                    return False
        else:
            logger.info("‚úÖ Pas de migration n√©cessaire")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la migration des donn√©es: {e}")
        return False

def ensure_all_columns(engine):
    """S'assure que toutes les colonnes n√©cessaires existent"""
    logger.info("V√©rification de toutes les colonnes...")
    
    required_columns = {
        'id': 'SERIAL PRIMARY KEY',
        'task_id': 'VARCHAR(36) UNIQUE NOT NULL',
        'urls': 'JSON NOT NULL',
        'status': 'VARCHAR(20) DEFAULT \'pending\'',
        'progress': 'JSON DEFAULT \'{"current": 0, "total": 1, "percentage": 0.0, "display": "0/1"}\'::json',
        'analysis_type': 'VARCHAR(50) DEFAULT \'standard\'',
        'result': 'JSON',
        'results': 'JSON DEFAULT \'[]\'::json',
        'error': 'TEXT',
        'created_at': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP',
        'started_at': 'TIMESTAMP',
        'completed_at': 'TIMESTAMP',
        'parameters': 'JSON',
        'callback_url': 'VARCHAR(512)',
        'priority': 'INTEGER DEFAULT 0',
        'metrics': 'JSON'
    }
    
    success = True
    for column_name, column_def in required_columns.items():
        if column_name in ['id']:  # Skip primary key
            continue
            
        # Extraire le type et la valeur par d√©faut
        if 'DEFAULT' in column_def:
            parts = column_def.split('DEFAULT')
            column_type = parts[0].strip()
            default_value = parts[1].strip().strip("'")
            
            # Pour les JSON, ne pas mettre de guillemets autour de la valeur par d√©faut
            if column_type == 'JSON':
                if not check_and_add_column(engine, 'scraping_tasks', column_name, column_type):
                    success = False
                else:
                    # D√©finir la valeur par d√©faut apr√®s cr√©ation
                    try:
                        with engine.connect() as conn:
                            conn.execute(text(f"ALTER TABLE scraping_tasks ALTER COLUMN {column_name} SET DEFAULT '{default_value}'::json"))
                            conn.commit()
                    except Exception:
                        pass  # Ignore si d√©j√† d√©fini
            else:
                if not check_and_add_column(engine, 'scraping_tasks', column_name, column_type, default_value):
                    success = False
        else:
            column_type = column_def
            if not check_and_add_column(engine, 'scraping_tasks', column_name, column_type):
                success = False
    
    return success

def test_table_operations(engine):
    """Test les op√©rations de base sur la table"""
    logger.info("Test des op√©rations sur la table...")
    
    try:
        with engine.connect() as conn:
            # Test d'insertion
            test_data = {
                'task_id': 'test-migration-12345',
                'urls': json.dumps(["http://test.com"]),
                'status': 'pending',
                'progress': json.dumps({"current": 0, "total": 1, "percentage": 0.0, "display": "0/1"}),
                'analysis_type': 'standard',
                'results': json.dumps([]),
                'priority': 0
            }
            
            # Supprimer s'il existe d√©j√†
            conn.execute(text("DELETE FROM scraping_tasks WHERE task_id = :task_id"), 
                        {"task_id": test_data['task_id']})
            
            # Ins√©rer
            result = conn.execute(text("""
                INSERT INTO scraping_tasks 
                (task_id, urls, status, progress, analysis_type, results, priority, created_at)
                VALUES 
                (:task_id, :urls, :status, :progress, :analysis_type, :results, :priority, NOW())
                RETURNING id
            """), test_data)
            
            inserted_id = result.fetchone()[0]
            
            # Test de lecture
            read_result = conn.execute(text("""
                SELECT task_id, status, progress, results 
                FROM scraping_tasks 
                WHERE id = :id
            """), {"id": inserted_id})
            
            row = read_result.fetchone()
            
            # Nettoyer
            conn.execute(text("DELETE FROM scraping_tasks WHERE id = :id"), {"id": inserted_id})
            conn.commit()
            
            logger.info("‚úÖ Test des op√©rations r√©ussi")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du test: {e}")
        return False

def main():
    """Fonction principale"""
    logger.info("üöÄ MIGRATION DE LA BASE DE DONN√âES")
    logger.info("=" * 50)
    
    try:
        # Connexion √† la base de donn√©es
        database_url = get_database_url()
        engine = create_engine(database_url, echo=False)
        
        # Test de connexion
        with engine.connect() as conn:
            result = conn.execute(text("SELECT version()"))
            version = result.fetchone()[0]
            logger.info(f"‚úÖ Connect√© √† PostgreSQL: {version.split()[0]} {version.split()[1]}")
        
        # V√©rifier que la table existe
        inspector = inspect(engine)
        if 'scraping_tasks' not in inspector.get_table_names():
            logger.error("‚ùå Table 'scraping_tasks' n'existe pas!")
            return 1
        
        # 1. S'assurer que toutes les colonnes existent
        if not ensure_all_columns(engine):
            logger.error("‚ùå √âchec de la v√©rification des colonnes")
            return 1
        
        # 2. Migrer les donn√©es de progress si n√©cessaire
        if not migrate_progress_data(engine):
            logger.error("‚ùå √âchec de la migration des donn√©es")
            return 1
        
        # 3. Tester les op√©rations
        if not test_table_operations(engine):
            logger.error("‚ùå √âchec du test des op√©rations")
            return 1
        
        logger.info("=" * 50)
        logger.info("‚úÖ MIGRATION TERMIN√âE AVEC SUCC√àS!")
        logger.info("\nüìã Actions recommand√©es:")
        logger.info("1. Red√©marrez vos services: docker-compose restart")
        logger.info("2. Testez votre API: POST /scrape")
        logger.info("3. V√©rifiez: GET /tasks")
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå ERREUR CRITIQUE: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())