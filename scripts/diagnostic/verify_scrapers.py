#!/usr/bin/env python3
"""
Script de v√©rification pour s'assurer que les nouveaux scrapers sont charg√©s
"""

import sys
import os
sys.path.append('/app')

def check_scrapers():
    """V√©rifie quels scrapers sont actuellement charg√©s"""
    print("üîç V√âRIFICATION DES SCRAPERS CHARG√âS")
    print("=" * 50)
    
    try:
        # Test du scraper traditionnel
        from app.scrapers.traditional import TunisianWebScraper
        scraper = TunisianWebScraper()
        
        print("üìä SCRAPER TRADITIONNEL:")
        
        # V√©rifier les m√©thodes sp√©cifiques aux nouveaux scrapers
        new_methods = [
            '_build_universal_patterns',
            '_extract_target_indicators_universal', 
            '_extract_by_recognized_units',
            '_universal_scrape'
        ]
        
        old_methods = [
            '_scrape_ins_with_context',
            '_extract_with_full_context',
            'contextual_patterns'
        ]
        
        new_count = 0
        for method in new_methods:
            has_method = hasattr(scraper, method)
            print(f"   {'‚úÖ' if has_method else '‚ùå'} {method}: {'OUI' if has_method else 'NON'}")
            if has_method:
                new_count += 1
        
        old_count = 0
        for method in old_methods:
            has_method = hasattr(scraper, method)
            print(f"   {'‚ùå' if has_method else '‚úÖ'} {method} (ancien): {'OUI' if has_method else 'NON'}")
            if has_method:
                old_count += 1
        
        print(f"\nüìà SCORE NOUVEAU SCRAPER: {new_count}/{len(new_methods)}")
        print(f"üìâ SCORE ANCIEN SCRAPER: {old_count}/{len(old_methods)}")
        
        # Test du scraper intelligent
        from app.scrapers.intelligent import IntelligentScraper
        intelligent = IntelligentScraper()
        
        print("\nüß† SCRAPER INTELLIGENT:")
        
        intelligent_new_methods = [
            '_prepare_intelligent_patterns',
            '_perform_intelligent_universal_analysis',
            '_enrich_values_with_intelligence'
        ]
        
        intelligent_new_count = 0
        for method in intelligent_new_methods:
            has_method = hasattr(intelligent, method)
            print(f"   {'‚úÖ' if has_method else '‚ùå'} {method}: {'OUI' if has_method else 'NON'}")
            if has_method:
                intelligent_new_count += 1
        
        print(f"\nüìà SCORE NOUVEAU INTELLIGENT: {intelligent_new_count}/{len(intelligent_new_methods)}")
        
        # Test settings
        from app.config.settings import settings
        print(f"\n‚öôÔ∏è SETTINGS:")
        print(f"   üìä TARGET_INDICATORS: {len(settings.TARGET_INDICATORS)} cat√©gories")
        print(f"   üí± RECOGNIZED_UNITS: {len(settings.RECOGNIZED_UNITS)} unit√©s")
        print(f"   üìÖ TARGET_YEARS: {len(settings.TARGET_YEARS)} ann√©es")
        
        # Conclusion
        total_new = new_count + intelligent_new_count
        total_possible = len(new_methods) + len(intelligent_new_methods)
        
        print(f"\nüéØ R√âSULTAT GLOBAL:")
        print(f"   M√©thodes nouvelles: {total_new}/{total_possible}")
        print(f"   M√©thodes anciennes: {old_count}/{len(old_methods)}")
        
        if total_new == total_possible and old_count == 0:
            print(f"\nüéâ PARFAIT ! NOUVEAUX SCRAPERS UNIVERSELS ACTIFS")
            return True
        elif total_new > total_possible // 2:
            print(f"\n‚ö†Ô∏è PARTIELLEMENT CHARG√â - Red√©marrage n√©cessaire")
            return False
        else:
            print(f"\n‚ùå ANCIENS SCRAPERS ENCORE ACTIFS - Fichiers non mis √† jour")
            return False
            
    except Exception as e:
        print(f"‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_quick_extraction():
    """Test rapide d'extraction avec les nouveaux scrapers"""
    print(f"\nüß™ TEST D'EXTRACTION RAPIDE")
    print("=" * 30)
    
    try:
        from app.scrapers.traditional import TunisianWebScraper
        
        scraper = TunisianWebScraper()
        
        # Test HTML simple avec des donn√©es √©conomiques
        test_html = """
        <html>
        <head><title>Test INS - Statistiques</title></head>
        <body>
            <h1>Indicateurs √âconomiques Tunisie</h1>
            <p>Inflation : 6,7% en juillet 2024</p>
            <p>PIB : 125000 millions de dinars en 2024</p>
            <p>Exportations : 15000 MD au premier trimestre</p>
            <table>
                <tr><th>Indicateur</th><th>Valeur</th></tr>
                <tr><td>Ch√¥mage</td><td>15,3%</td></tr>
                <tr><td>Population</td><td>12000000 habitants</td></tr>
            </table>
        </body>
        </html>
        """
        
        # Test avec le nouveau scraper universel
        result = scraper._universal_scrape(test_html, "https://test.ins.tn/stats")
        
        if result and result.structured_data.get('extracted_values'):
            values = result.structured_data['extracted_values']
            print(f"‚úÖ EXTRACTION R√âUSSIE: {len(values)} valeurs")
            
            # Afficher quelques valeurs
            for i, (key, value) in enumerate(list(values.items())[:3]):
                indicator = value.get('indicator_name', 'N/A')
                val = value.get('value', 0)
                unit = value.get('unit', '')
                method = value.get('extraction_method', '')
                print(f"   {i+1}. {indicator}: {val} {unit} [{method}]")
            
            # V√©rifier les m√©tadonn√©es universelles
            summary = result.structured_data.get('extraction_summary', {})
            if summary:
                print(f"   üìä Target indicators: {summary.get('target_indicators_matched', 0)}")
                print(f"   üìä Categories found: {summary.get('categories_found', 0)}")
            
            return True
        else:
            print("‚ùå AUCUNE VALEUR EXTRAITE")
            return False
            
    except Exception as e:
        print(f"‚ùå ERREUR DE TEST: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Fonction principale"""
    print("üîç DIAGNOSTIC COMPLET DES SCRAPERS")
    print("=" * 60)
    
    scrapers_ok = check_scrapers()
    extraction_ok = test_quick_extraction() if scrapers_ok else False
    
    print(f"\n" + "=" * 60)
    print("üéØ DIAGNOSTIC FINAL:")
    print(f"   üì¶ Scrapers charg√©s: {'‚úÖ OUI' if scrapers_ok else '‚ùå NON'}")
    print(f"   üîß Extraction fonctionnelle: {'‚úÖ OUI' if extraction_ok else '‚ùå NON'}")
    
    if scrapers_ok and extraction_ok:
        print(f"\nüéâ TOUT FONCTIONNE ! Les scrapers universels sont actifs.")
        print(f"üöÄ Votre t√¢che bloqu√©e devrait maintenant se terminer correctement.")
    else:
        print(f"\n‚ö†Ô∏è PROBL√àME D√âTECT√â:")
        if not scrapers_ok:
            print(f"   - Les nouveaux fichiers ne sont pas charg√©s")
            print(f"   - Solution: docker-compose build --no-cache")
        if not extraction_ok:
            print(f"   - L'extraction ne fonctionne pas")
            print(f"   - V√©rifiez les imports et erreurs ci-dessus")
    
    return 0 if (scrapers_ok and extraction_ok) else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)